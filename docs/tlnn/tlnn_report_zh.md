\# 两层全连接神经网络 MNIST 实验报告



\## 实验概述

本实验实现了一个两层全连接神经网络（输入层784节点 → 隐藏层128节点 → 输出层10节点），在MNIST手写数字数据集上进行训练和测试。



\## 模型配置

\- \*\*网络结构\*\*: 784-128-10

\- \*\*激活函数\*\*: 隐藏层使用Sigmoid，输出层使用Softmax

\- \*\*损失函数\*\*: 交叉熵损失

\- \*\*优化算法\*\*: 小批量梯度下降

\- \*\*超参数\*\*: 

&nbsp; - 学习率: 0.5

&nbsp; - 批量大小: 128

&nbsp; - 训练轮数: 100



\## 训练结果



\### 损失下降曲线

!\[训练损失曲线](results/Figure\_1.png)



训练过程显示损失函数稳定下降：

\- 初始损失: 0.736

\- 最终损失: 0.005

\- 训练耗时: 167.73秒



\### 测试准确率

\*\*最终测试准确率: 98.01%\*\*



在10,000个测试样本上达到98.01%的准确率，表明模型具有良好的泛化能力。



\## 预测效果展示

!\[预测结果可视化](results/Figure\_2.png)



随机抽取的15个测试样本预测结果显示：

\- 所有样本均预测正确（绿色标签）

\- 模型能够准确识别各种手写数字风格

\- 对不同书写角度和粗细的数字都有良好识别能力



\## 结论

该两层全连接神经网络在MNIST数据集上表现优秀：

1\. 训练过程稳定，损失收敛良好

2\. 测试准确率达到98.01%，接近业界基准

3\. 模型具有良好的泛化能力和鲁棒性

4\. 证明了简单神经网络在图像分类任务中的有效性



实验成功验证了神经网络基本原理的正确性，包括前向传播、反向传播和梯度下降等核心算法。

